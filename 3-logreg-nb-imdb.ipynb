{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Classification the old-fashioned way: \n",
    "## `Naive Bayes`, `Logistic Regression`, and `Ngrams`\n",
    "\n",
    "\n",
    "I don't use `fastai` library, therefore this notebook is my own version using more common libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to show how sentiment classification is done via the classic techniques of `Naive Bayes`, `Logistic regression`, and `Ngrams`.  We will be using `sklearn` and the `fastai` library.\n",
    "\n",
    "In a future lesson, we will revisit sentiment classification using `deep learning`, so that you can compare the two approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content here was extended from [Lesson 10 of the fast.ai Machine Learning course](https://course.fast.ai/lessonsml1/lesson10.html). Linear model is pretty close to the state of the art here.  Jeremy surpassed state of the art using a RNN in fall 2017."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.The fastai library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will begin using [the fastai library](https://docs.fast.ai) (version 1.0) in this notebook.  We will use it more once we move on to neural networks.\n",
    "\n",
    "The fastai library is built on top of PyTorch and encodes many state-of-the-art best practices. It is used in production at a number of companies.  You can read more about it here:\n",
    "\n",
    "- [Fast.ai's software could radically democratize AI](https://www.zdnet.com/article/fast-ais-new-software-could-radically-democratize-ai/) (ZDNet)\n",
    "\n",
    "- [fastai v1 for PyTorch: Fast and accurate neural nets using modern best practices](https://www.fast.ai/2018/10/02/fastai-ai/) (fast.ai)\n",
    "\n",
    "- [fastai docs](https://docs.fast.ai/)\n",
    "\n",
    "### Installation\n",
    "\n",
    "With conda:\n",
    "\n",
    "`conda install -c pytorch -c fastai fastai=1.0`\n",
    "\n",
    "Or with pip:\n",
    "\n",
    "`pip install fastai==1.0`\n",
    "\n",
    "More [installation information here](https://github.com/fastai/fastai/blob/master/README.md).\n",
    "\n",
    "Beginning in lesson 4, we will be using GPUs, so if you want, you could switch to a [cloud option](https://course.fast.ai/#using-a-gpu) now to setup fastai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The IMDB dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"IMDb.png\" alt=\"floating point\" style=\"width: 90%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [large movie review dataset](http://ai.stanford.edu/~amaas/data/sentiment/) contains a collection of 50,000 reviews from IMDB, We will use the version hosted as part [fast.ai datasets](https://course.fast.ai/datasets.html) on AWS Open Datasets. \n",
    "\n",
    "The dataset contains an even number of positive and negative reviews. The authors considered only highly polarized reviews. A negative review has a score ≤ 4 out of 10, and a positive review has a score ≥ 7 out of 10. Neutral reviews are not included in the dataset. The dataset is divided into training and test sets. The training set is the same 25,000 labeled reviews.\n",
    "\n",
    "The **sentiment classification task** consists of predicting the polarity (positive or negative) of a given text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import sklearn.feature_extraction.text as sklearn_text\n",
    "import pickle \n",
    "from sklearn.model_selection import train_test_split\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preview the sample IMDb data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "fast.ai has a number of [datasets hosted via AWS Open Datasets](https://course.fast.ai/datasets.html) for easy download. We can see them by checking the docs for URLs (remember `??` is a helpful command):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always good to start working on a sample of your data before you use the full dataset-- this allows for quicker computations as you debug and get your code working. For IMDB, there is a sample dataset already available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mbritish-fiction-corpus\u001b[m\u001b[m movie_data.csv\r\n"
     ]
    }
   ],
   "source": [
    "path = '../data/fastai/'\n",
    "!ls {path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(f'{path}/movie_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the movie reviews from the sample IMDb data set.\n",
    "\n",
    "My approach is completely different from the one in the lesson. Fastai library has gone through a lot of changes and the code is no longe relevant. Better using more common code so to have better support from the community too."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Transform/remove punctuation characters. \n",
    "    \n",
    "    :param\n",
    "    text : str\n",
    "        Original text.\n",
    "    \n",
    "    :return\n",
    "    text : str\n",
    "        Text without punctuation.\n",
    "    \"\"\"\n",
    "    table = str.maketrans('','', string.punctuation)\n",
    "    return text.translate(table).strip()\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: remove_punctuation(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974 the teenager Martha Moxley Maggie Grac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK so I really like Kris Kristofferson and his...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SPOILER Do not read this if you think about wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD forgetting just how ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974 the teenager Martha Moxley Maggie Grac...          1\n",
       "1  OK so I really like Kris Kristofferson and his...          0\n",
       "2  SPOILER Do not read this if you think about wa...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD forgetting just how ...          0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in 1974 the teenager martha moxley maggie grac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok so i really like kris kristofferson and his...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spoiler do not read this if you think about wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i recently bought the dvd forgetting just how ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  in 1974 the teenager martha moxley maggie grac...          1\n",
       "1  ok so i really like kris kristofferson and his...          0\n",
       "2  spoiler do not read this if you think about wa...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  i recently bought the dvd forgetting just how ...          0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_vectorizer(data):\n",
    "    vectorizer = CountVectorizer()\n",
    "    embeddings = vectorizer.fit_transform(data)\n",
    "    return embeddings, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>in 1974 the teenager martha moxley maggie grac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ok so i really like kris kristofferson and his...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spoiler do not read this if you think about wa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>i recently bought the dvd forgetting just how ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  in 1974 the teenager martha moxley maggie grac...          1\n",
       "1  ok so i really like kris kristofferson and his...          0\n",
       "2  spoiler do not read this if you think about wa...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  i recently bought the dvd forgetting just how ...          0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count, vectorizer = count_vectorizer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40000, 158461)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That means there are 40'000 review, and there are 158461 words in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_count = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 158461)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_count.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`x_train_count` is a sparse matrix (in the lesson has been explained the various ways of storing a sparse matrix) but it can be represented numerically only if you use `todense()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5479461"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(X_train_count.todense())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first row in the matrix seems to have 92 non-zero entries. This would also mean that the first review contains 92 unique words. Is this correct?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "nnz = np.count_nonzero(X_train_count.todense()[0,:])\n",
    "x_train_rows = X_train_count.shape[0]\n",
    "sparsity = (x_train_rows - nnz) / x_train_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9977"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparsity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "92"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(X_train_count.todense()[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for word in df.review[0].split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'teenager', 'to', 'on', 'dramatization', 'moved', 'support', 'net', 'criminal', 'fifteen', 'do', 'in', 'his', 'christopher', 'wealthy', 'the', 'days', 'br', '1974', 'haven', 'perjurer', 'mitchell', 'who', 'remained', 'and', 'kennedy', 'has', 'connecticut', 'is', 'moves', 'used', 'robert', 'shows', 'twentytwo', 'was', 'house', 'influence', 'my', 'maggie', 'but', 'former', 'stephen', 'night', 'story', 'murdered', 'how', 'parallel', 'murder', 'movie', 'crime', 'years', 'money', 'power', 'a', 'committed', 'family', 'welcome', 'than', 'there', 'good', 'highclass', 'weeks', 'murderbr', 'writing', 'however', 'eve', 'book', 'old', 'halloween', 'more', 'retired', 'la', 'discover', 'tv', 'backyard', 'trial', 'disgrace', 'girl', 'decides', 'mischief', 'available', 'fallen', 'oj', 'last', 'case', 'by', 'later', 'disclose', 'hideous', 'convicted', 'meloni', 'squirm', 'fuhrman', 'that', 'snoopy', 'forster', 'not', 'purpose', 'their', 'writer', 'title', 'greenwich', 'grace', 'charge', 'true', 'lack', 'idaho', 'unsolved', 'them', 'perjury', 'steve', 'twenty', 'locals', 'belle', '70s', 'mark', 'able', 'partner', 'they', 'she', 'brazil', 'whose', 'for', 'cover', 'screenplay', 'moxley', 'andrew', 'detective', 'emotion', 'of', 'powerful', 'carroll', 'sevenbr', 'with', 'simpson', 'area', 'vote', 'her', 'mother', 'investigate', 'investigation', 'rich', 'martha'}\n"
     ]
    }
   ],
   "source": [
    "print(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert string to index\n",
    "\n",
    "Mind there is a manual way, or an _attribute_ of sklearn can give you what you want. Both are shown:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in 1974 the teenager martha moxley maggie grace moves to the highclass area of belle haven greenwich connecticut on the mischief night eve of halloween she was murdered in the backyard of her house and her murder remained unsolved twentytwo years later the writer mark fuhrman christopher meloni who is a former la detective that has fallen in disgrace for perjury in oj simpson trial and moved to idaho decides to investigate the case with his partner stephen weeks andrew mitchell with the purpose of writing a book the locals squirm and do not welcome them but with the support of the retired detective steve carroll robert forster that was in charge of the investigation in the 70s they discover the criminal and a net of power and money to cover the murderbr br murder in greenwich is a good tv movie with the true story of a murder of a fifteen years old girl that was committed by a wealthy teenager whose mother was a kennedy the powerful and rich family used their influence to cover the murder for more than twenty years however a snoopy detective and convicted perjurer in disgrace was able to disclose how the hideous crime was committed the screenplay shows the investigation of mark and the last days of martha in parallel but there is a lack of the emotion in the dramatization my vote is sevenbr br title brazil not available'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_name = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000000000001',\n",
       " '000001',\n",
       " '0001',\n",
       " '00015',\n",
       " '001',\n",
       " '0010',\n",
       " '002',\n",
       " '00383042',\n",
       " '006',\n",
       " '0069',\n",
       " '007',\n",
       " '0079',\n",
       " '007br',\n",
       " '007s',\n",
       " '0080',\n",
       " '0083',\n",
       " '009',\n",
       " '00agent',\n",
       " '00s',\n",
       " '00schneider',\n",
       " '01',\n",
       " '010',\n",
       " '01000',\n",
       " '010707',\n",
       " '010br',\n",
       " '010makes',\n",
       " '0110',\n",
       " '012310',\n",
       " '0130',\n",
       " '013007',\n",
       " '02',\n",
       " '0205',\n",
       " '0210',\n",
       " '0230',\n",
       " '029',\n",
       " '02br',\n",
       " '03',\n",
       " '030',\n",
       " '03092005',\n",
       " '0310',\n",
       " '039',\n",
       " '03oct2009',\n",
       " '04',\n",
       " '04082007',\n",
       " '041',\n",
       " '044',\n",
       " '048',\n",
       " '05',\n",
       " '050',\n",
       " '0510',\n",
       " '053105',\n",
       " '05br',\n",
       " '06',\n",
       " '060241',\n",
       " '0615',\n",
       " '06and',\n",
       " '06br',\n",
       " '07',\n",
       " '075',\n",
       " '08',\n",
       " '081006',\n",
       " '087',\n",
       " '08th',\n",
       " '09',\n",
       " '09082009',\n",
       " '09br',\n",
       " '0br',\n",
       " '0clock',\n",
       " '0f',\n",
       " '0ne',\n",
       " '0r',\n",
       " '0s',\n",
       " '0stars',\n",
       " '0ttbr',\n",
       " '0when',\n",
       " '10',\n",
       " '100',\n",
       " '1000',\n",
       " '10000',\n",
       " '100000',\n",
       " '1000000',\n",
       " '10000000',\n",
       " '100000000',\n",
       " '1000000000',\n",
       " '1000000000000',\n",
       " '1000000000000010',\n",
       " '1000000000000010000000000000',\n",
       " '100000dm',\n",
       " '100001',\n",
       " '10002000',\n",
       " '10005000',\n",
       " '1000lb',\n",
       " '1000month',\n",
       " '1000s',\n",
       " '1000th',\n",
       " '1000word',\n",
       " '1000wordstory',\n",
       " '1001',\n",
       " '100100',\n",
       " '100200',\n",
       " '100219',\n",
       " '1004',\n",
       " '100603',\n",
       " '100am',\n",
       " '100b',\n",
       " '100br',\n",
       " '100bt',\n",
       " '100ibs',\n",
       " '100if',\n",
       " '100k',\n",
       " '100kin',\n",
       " '100m',\n",
       " '100miles',\n",
       " '100millionplus',\n",
       " '100min',\n",
       " '100minute',\n",
       " '100minutelong',\n",
       " '100mph',\n",
       " '100percent',\n",
       " '100plus',\n",
       " '100s',\n",
       " '100square',\n",
       " '100th',\n",
       " '100thgrade',\n",
       " '100x',\n",
       " '100yards',\n",
       " '100year',\n",
       " '100yearold',\n",
       " '100years',\n",
       " '101',\n",
       " '1010',\n",
       " '10102009',\n",
       " '1010br',\n",
       " '1010i',\n",
       " '1010negative',\n",
       " '1010seek',\n",
       " '1011',\n",
       " '1012',\n",
       " '1013',\n",
       " '1014',\n",
       " '101499',\n",
       " '1015',\n",
       " '101503',\n",
       " '101575',\n",
       " '101br',\n",
       " '101minute',\n",
       " '101st',\n",
       " '101year',\n",
       " '101yearold',\n",
       " '102',\n",
       " '1020',\n",
       " '1020000',\n",
       " '102030',\n",
       " '10241930',\n",
       " '1025',\n",
       " '10282007',\n",
       " '102862',\n",
       " '10292006',\n",
       " '102955',\n",
       " '102nd',\n",
       " '103',\n",
       " '1030',\n",
       " '1030pm',\n",
       " '103104',\n",
       " '104',\n",
       " '1040',\n",
       " '1040a',\n",
       " '105',\n",
       " '1050',\n",
       " '10531st',\n",
       " '10556',\n",
       " '105lbs',\n",
       " '105min',\n",
       " '106',\n",
       " '107',\n",
       " '1072007',\n",
       " '108',\n",
       " '1080',\n",
       " '1080p',\n",
       " '10852',\n",
       " '109',\n",
       " '1095',\n",
       " '1099',\n",
       " '109minute',\n",
       " '10am',\n",
       " '10and',\n",
       " '10as',\n",
       " '10br',\n",
       " '10but',\n",
       " '10centsadance',\n",
       " '10check',\n",
       " '10day',\n",
       " '10did',\n",
       " '10dimensional',\n",
       " '10dirarne',\n",
       " '10dirbrad',\n",
       " '10dirbrett',\n",
       " '10diremmanuel',\n",
       " '10direwald',\n",
       " '10dirhenri',\n",
       " '10dirjesus',\n",
       " '10dirjim',\n",
       " '10dirjohn',\n",
       " '10dirjon',\n",
       " '10dirleon',\n",
       " '10dirmario',\n",
       " '10dirmick',\n",
       " '10dirstefan',\n",
       " '10dirsteve',\n",
       " '10dirsydney',\n",
       " '10dirtobe',\n",
       " '10disc',\n",
       " '10easy',\n",
       " '10foot',\n",
       " '10footbargepole',\n",
       " '10ft',\n",
       " '10highly',\n",
       " '10hour',\n",
       " '10i',\n",
       " '10in',\n",
       " '10its',\n",
       " '10just',\n",
       " '10k',\n",
       " '10line',\n",
       " '10master',\n",
       " '10min',\n",
       " '10mins',\n",
       " '10minute',\n",
       " '10movie',\n",
       " '10no',\n",
       " '10now',\n",
       " '10odd',\n",
       " '10of10',\n",
       " '10one',\n",
       " '10page',\n",
       " '10pm',\n",
       " '10ps',\n",
       " '10s',\n",
       " '10scale',\n",
       " '10second',\n",
       " '10speed',\n",
       " '10star',\n",
       " '10th',\n",
       " '10thbr',\n",
       " '10the',\n",
       " '10they',\n",
       " '10this',\n",
       " '10times',\n",
       " '10to1',\n",
       " '10um',\n",
       " '10well',\n",
       " '10what',\n",
       " '10which',\n",
       " '10x',\n",
       " '10x10',\n",
       " '10xs',\n",
       " '10year',\n",
       " '10yearold',\n",
       " '10yearolds',\n",
       " '10yearsold',\n",
       " '10yo',\n",
       " '10yr',\n",
       " '10yrold',\n",
       " '10yrs',\n",
       " '11',\n",
       " '110',\n",
       " '1100',\n",
       " '11000',\n",
       " '110000th',\n",
       " '11001001',\n",
       " '1100ad',\n",
       " '1100am',\n",
       " '1100th',\n",
       " '11072004',\n",
       " '110br',\n",
       " '110jd',\n",
       " '110minute',\n",
       " '110s',\n",
       " '110th',\n",
       " '111',\n",
       " '1110',\n",
       " '1111',\n",
       " '111108',\n",
       " '111128',\n",
       " '1112',\n",
       " '1112009',\n",
       " '1113',\n",
       " '1114',\n",
       " '11146',\n",
       " '1115ambut',\n",
       " '111but',\n",
       " '111minute',\n",
       " '112',\n",
       " '112001sandler',\n",
       " '112003',\n",
       " '1121',\n",
       " '112101',\n",
       " '112291',\n",
       " '11240',\n",
       " '112413',\n",
       " '112443',\n",
       " '112902',\n",
       " '113',\n",
       " '1130',\n",
       " '1130pm',\n",
       " '1131516',\n",
       " '1138',\n",
       " '1139',\n",
       " '113min',\n",
       " '114',\n",
       " '115',\n",
       " '1150',\n",
       " '115am',\n",
       " '116',\n",
       " '11673',\n",
       " '116minute',\n",
       " '116minutes',\n",
       " '116th',\n",
       " '117',\n",
       " '11706',\n",
       " '1172002',\n",
       " '117s',\n",
       " '118',\n",
       " '119',\n",
       " '1192br',\n",
       " '1195',\n",
       " '119minute',\n",
       " '11br',\n",
       " '11minute',\n",
       " '11stars',\n",
       " '11th',\n",
       " '11year',\n",
       " '11yearold',\n",
       " '11yearsold',\n",
       " '11yr',\n",
       " '12',\n",
       " '120',\n",
       " '1200',\n",
       " '12000',\n",
       " '12000000',\n",
       " '1200000000000',\n",
       " '1200f',\n",
       " '1200pm',\n",
       " '1201',\n",
       " '1201pm',\n",
       " '1202',\n",
       " '120kmph',\n",
       " '120page',\n",
       " '121',\n",
       " '1210',\n",
       " '12100',\n",
       " '121031',\n",
       " '12106',\n",
       " '1213',\n",
       " '12134',\n",
       " '12139',\n",
       " '1214',\n",
       " '121444',\n",
       " '1215',\n",
       " '1216',\n",
       " '121699',\n",
       " '121am',\n",
       " '122',\n",
       " '12206',\n",
       " '12211',\n",
       " '12242009',\n",
       " '12262008',\n",
       " '1227',\n",
       " '12272006',\n",
       " '1229',\n",
       " '123',\n",
       " '1230pm',\n",
       " '123105',\n",
       " '12312006',\n",
       " '1234',\n",
       " '12345',\n",
       " '1235',\n",
       " '12383499143743701',\n",
       " '123867',\n",
       " '1242',\n",
       " '1245119',\n",
       " '1245am',\n",
       " '1249',\n",
       " '125',\n",
       " '12500',\n",
       " '125000',\n",
       " '125150',\n",
       " '125m',\n",
       " '127',\n",
       " '1272002',\n",
       " '127minute',\n",
       " '128',\n",
       " '12802',\n",
       " '1282001',\n",
       " '128minute',\n",
       " '12a',\n",
       " '12always',\n",
       " '12br',\n",
       " '12episode',\n",
       " '12foot',\n",
       " '12h',\n",
       " '12hour',\n",
       " '12hours',\n",
       " '12hr',\n",
       " '12i',\n",
       " '12inch',\n",
       " '12lane',\n",
       " '12m',\n",
       " '12man',\n",
       " '12minute',\n",
       " '12mm',\n",
       " '12nd',\n",
       " '12out',\n",
       " '12p',\n",
       " '12pack',\n",
       " '12s',\n",
       " '12th',\n",
       " '12thgrade',\n",
       " '12thrate',\n",
       " '12thrateat',\n",
       " '12timesovergreat',\n",
       " '12x',\n",
       " '12year',\n",
       " '12yearold',\n",
       " '12yearolds',\n",
       " '12years',\n",
       " '12yr',\n",
       " '12yrold',\n",
       " '12yrs',\n",
       " '13',\n",
       " '130',\n",
       " '1300',\n",
       " '13000',\n",
       " '130000',\n",
       " '1300s',\n",
       " '130am',\n",
       " '131',\n",
       " '1310',\n",
       " '1314',\n",
       " '1317',\n",
       " '132',\n",
       " '133',\n",
       " '1331',\n",
       " '1331br',\n",
       " '1335',\n",
       " '1336th',\n",
       " '134',\n",
       " '134minute',\n",
       " '135',\n",
       " '13516',\n",
       " '135m',\n",
       " '136',\n",
       " '137',\n",
       " '1371br',\n",
       " '138',\n",
       " '13848',\n",
       " '138minute',\n",
       " '139',\n",
       " '13and',\n",
       " '13br',\n",
       " '13but',\n",
       " '13episode',\n",
       " '13it',\n",
       " '13k',\n",
       " '13minute',\n",
       " '13part',\n",
       " '13rd',\n",
       " '13s',\n",
       " '13th',\n",
       " '13thand',\n",
       " '13thbr',\n",
       " '13thcentury',\n",
       " '13thhalloween',\n",
       " '13thi',\n",
       " '13ththe',\n",
       " '13thversions',\n",
       " '13thwhats',\n",
       " '13thwith',\n",
       " '13year',\n",
       " '13yearold',\n",
       " '13yearolds',\n",
       " '14',\n",
       " '140',\n",
       " '1400',\n",
       " '1408',\n",
       " '140hp',\n",
       " '140minute',\n",
       " '141',\n",
       " '1410',\n",
       " '1415',\n",
       " '1415yearold',\n",
       " '1416',\n",
       " '1416s',\n",
       " '1417',\n",
       " '1424',\n",
       " '142cuz',\n",
       " '142m',\n",
       " '142minute',\n",
       " '143',\n",
       " '1430',\n",
       " '144',\n",
       " '14400',\n",
       " '144mb',\n",
       " '145',\n",
       " '1453',\n",
       " '1454',\n",
       " '145th',\n",
       " '146',\n",
       " '147',\n",
       " '1473',\n",
       " '1479',\n",
       " '147br',\n",
       " '14800',\n",
       " '1489',\n",
       " '149',\n",
       " '1490s',\n",
       " '1492',\n",
       " '1492br',\n",
       " '1498',\n",
       " '1499',\n",
       " '14br',\n",
       " '14goingon9',\n",
       " '14ieme',\n",
       " '14inch',\n",
       " '14m',\n",
       " '14minute',\n",
       " '14th',\n",
       " '14x',\n",
       " '14year',\n",
       " '14yearold',\n",
       " '14yearolds',\n",
       " '14yo',\n",
       " '14yrold',\n",
       " '14ème',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '15000',\n",
       " '150000',\n",
       " '1500000',\n",
       " '15000000',\n",
       " '1500s',\n",
       " '150k',\n",
       " '150mile',\n",
       " '150minute',\n",
       " '151',\n",
       " '1510',\n",
       " '1516',\n",
       " '1516yearsold',\n",
       " '1517',\n",
       " '1518',\n",
       " '1520',\n",
       " '152000000',\n",
       " '1530',\n",
       " '1532',\n",
       " '154',\n",
       " '1547',\n",
       " '155',\n",
       " '155ºf',\n",
       " '156',\n",
       " '1561',\n",
       " '157',\n",
       " '157th',\n",
       " '1588',\n",
       " '15881679',\n",
       " '158minute',\n",
       " '1594',\n",
       " '1596',\n",
       " '1596jews',\n",
       " '1598i',\n",
       " '1599',\n",
       " '15apr08',\n",
       " '15as',\n",
       " '15br',\n",
       " '15bus',\n",
       " '15episode',\n",
       " '15h',\n",
       " '15hrs',\n",
       " '15k',\n",
       " '15mins',\n",
       " '15minute',\n",
       " '15minutes',\n",
       " '15pa',\n",
       " '15second',\n",
       " '15th',\n",
       " '15thcentury',\n",
       " '15the',\n",
       " '15to18',\n",
       " '15year',\n",
       " '15yearold',\n",
       " '15years',\n",
       " '16',\n",
       " '160',\n",
       " '1600',\n",
       " '16000',\n",
       " '1600s',\n",
       " '1602',\n",
       " '1604',\n",
       " '1606',\n",
       " '160lbs',\n",
       " '160page',\n",
       " '161',\n",
       " '1610',\n",
       " '1611',\n",
       " '1617',\n",
       " '1618',\n",
       " '1618br',\n",
       " '162',\n",
       " '1620',\n",
       " '1625',\n",
       " '1627',\n",
       " '163',\n",
       " '163000',\n",
       " '163minute',\n",
       " '164',\n",
       " '1647',\n",
       " '165',\n",
       " '165m',\n",
       " '166',\n",
       " '1660s',\n",
       " '167',\n",
       " '168',\n",
       " '1683',\n",
       " '169',\n",
       " '1690',\n",
       " '1690s',\n",
       " '1692',\n",
       " '169minute',\n",
       " '16bit',\n",
       " '16br',\n",
       " '16hour',\n",
       " '16k',\n",
       " '16minute',\n",
       " '16mm',\n",
       " '16mmstyle',\n",
       " '16month',\n",
       " '16rated',\n",
       " '16s',\n",
       " '16th',\n",
       " '16thcentury',\n",
       " '16year',\n",
       " '16yearold',\n",
       " '16yearolds',\n",
       " '16yrold',\n",
       " '16ème',\n",
       " '16éme',\n",
       " '17',\n",
       " '170',\n",
       " '1700',\n",
       " '17000',\n",
       " '1700s',\n",
       " '1700sbr',\n",
       " '170209',\n",
       " '1709',\n",
       " '1709s',\n",
       " '170x',\n",
       " '171',\n",
       " '1710',\n",
       " '1718',\n",
       " '1720',\n",
       " '172003',\n",
       " '1740s',\n",
       " '1745',\n",
       " '1746',\n",
       " '175',\n",
       " '1750',\n",
       " '17500',\n",
       " '1755',\n",
       " '1759',\n",
       " '176th',\n",
       " '177',\n",
       " '1775',\n",
       " '177583',\n",
       " '1776',\n",
       " '1780s',\n",
       " '1781',\n",
       " '1781br',\n",
       " '1789',\n",
       " '1790',\n",
       " '1790s',\n",
       " '1792',\n",
       " '1793',\n",
       " '1794',\n",
       " '179495',\n",
       " '1798',\n",
       " '17inch',\n",
       " '17minute',\n",
       " '17th',\n",
       " '17thcentury',\n",
       " '17year',\n",
       " '17yearold',\n",
       " '17yo',\n",
       " '18',\n",
       " '180',\n",
       " '1800',\n",
       " '18000',\n",
       " '18000000',\n",
       " '1800collect',\n",
       " '1800hrs',\n",
       " '1800s',\n",
       " '1800sbr',\n",
       " '1800smans',\n",
       " '18011844',\n",
       " '180d',\n",
       " '180minute',\n",
       " '180s',\n",
       " '1813',\n",
       " '1814',\n",
       " '1816',\n",
       " '1819',\n",
       " '1819th',\n",
       " '181minute',\n",
       " '1820',\n",
       " '1823',\n",
       " '1824',\n",
       " '1825',\n",
       " '1829',\n",
       " '182s',\n",
       " '183',\n",
       " '1830',\n",
       " '1830s',\n",
       " '1832',\n",
       " '1835',\n",
       " '1836',\n",
       " '1837',\n",
       " '18371898',\n",
       " '1839',\n",
       " '183minute',\n",
       " '1840',\n",
       " '18401911',\n",
       " '1840s',\n",
       " '1840sbr',\n",
       " '1842br',\n",
       " '1845',\n",
       " '1846',\n",
       " '1847',\n",
       " '1849',\n",
       " '185',\n",
       " '1850',\n",
       " '1850s',\n",
       " '1851',\n",
       " '1851br',\n",
       " '1852',\n",
       " '1853',\n",
       " '1854',\n",
       " '1857half',\n",
       " '1858',\n",
       " '1859',\n",
       " '18591860',\n",
       " '1860',\n",
       " '18601909',\n",
       " '18602',\n",
       " '1860s',\n",
       " '1861',\n",
       " '1862',\n",
       " '1863',\n",
       " '1864',\n",
       " '1865',\n",
       " '186566',\n",
       " '1866',\n",
       " '1867',\n",
       " '1868',\n",
       " '1869',\n",
       " '187',\n",
       " '1870',\n",
       " '187000000',\n",
       " '1870and',\n",
       " '1870s',\n",
       " '1871',\n",
       " '1872',\n",
       " '1873',\n",
       " '18741965',\n",
       " '1876',\n",
       " '1879',\n",
       " '1880',\n",
       " '1880s',\n",
       " '1881',\n",
       " '1886',\n",
       " '1888',\n",
       " '1889',\n",
       " '188minute',\n",
       " '188o',\n",
       " '1890',\n",
       " '1890s',\n",
       " '1890sbr',\n",
       " '1890sbut',\n",
       " '1891',\n",
       " '1892',\n",
       " '1893',\n",
       " '1894',\n",
       " '1894br',\n",
       " '1895',\n",
       " '18951898',\n",
       " '18951966',\n",
       " '1895br',\n",
       " '1896',\n",
       " '18961899',\n",
       " '18961982',\n",
       " '1896jeff',\n",
       " '1897',\n",
       " '1898',\n",
       " '1898br',\n",
       " '1899',\n",
       " '18br',\n",
       " '18rated',\n",
       " '18th',\n",
       " '18th19th',\n",
       " '18thcentury',\n",
       " '18year',\n",
       " '18yearold',\n",
       " '18yearolds',\n",
       " '18yearsold',\n",
       " '18yr',\n",
       " '19',\n",
       " '190',\n",
       " '1900',\n",
       " '19000000',\n",
       " '1900s',\n",
       " '1900sbr',\n",
       " '1900swow',\n",
       " '1901',\n",
       " '1902',\n",
       " '1902ii',\n",
       " '1903',\n",
       " '1904',\n",
       " '19041971',\n",
       " '1905',\n",
       " '1905br',\n",
       " '1906',\n",
       " '19061952',\n",
       " '19061979',\n",
       " '1907',\n",
       " '1908',\n",
       " '1909',\n",
       " '1910',\n",
       " '1910s',\n",
       " '1911',\n",
       " '191112',\n",
       " '1912',\n",
       " '1912br',\n",
       " '1913',\n",
       " '1914',\n",
       " '19141915',\n",
       " '1915',\n",
       " '1915br',\n",
       " '1916',\n",
       " '1917',\n",
       " '1918',\n",
       " '191820',\n",
       " '1918br',\n",
       " '1919',\n",
       " '192',\n",
       " '1920',\n",
       " '19201939',\n",
       " '19201998',\n",
       " '1920helping',\n",
       " '1920s',\n",
       " '1920sbr',\n",
       " '1920searly',\n",
       " '1921',\n",
       " '192122',\n",
       " '1922',\n",
       " '1922most',\n",
       " '1923',\n",
       " '19231994',\n",
       " '1923s',\n",
       " '1924',\n",
       " '1924kriemhild',\n",
       " '1925',\n",
       " '1926',\n",
       " '1927',\n",
       " '19271928',\n",
       " '1927s',\n",
       " '1928',\n",
       " '19281929',\n",
       " '19281932',\n",
       " '1928br',\n",
       " '1928s',\n",
       " '1929',\n",
       " '19291931',\n",
       " '192930',\n",
       " '192988',\n",
       " '1929his',\n",
       " '1929s',\n",
       " '1930',\n",
       " '19301934',\n",
       " '19301935',\n",
       " '193031',\n",
       " '193040',\n",
       " '1930and',\n",
       " '1930br',\n",
       " '1930comedy',\n",
       " '1930europe',\n",
       " '1930s',\n",
       " '1930s1940s',\n",
       " '1930s40s',\n",
       " '1930s50s',\n",
       " '1930saudiences',\n",
       " '1930sbecause',\n",
       " '1930sbr',\n",
       " '1930sera',\n",
       " '1930sish',\n",
       " '1930speter',\n",
       " '1930sset',\n",
       " '1930sstyle',\n",
       " '1930sthe',\n",
       " '1930sthis',\n",
       " '1930the',\n",
       " '1931',\n",
       " '19311997br',\n",
       " '193139',\n",
       " '1931br',\n",
       " '1931featured',\n",
       " '1931this',\n",
       " '1932',\n",
       " '19323',\n",
       " '193234',\n",
       " '1932pack',\n",
       " '1932s',\n",
       " '1933',\n",
       " '19331939',\n",
       " '19334',\n",
       " '1933s',\n",
       " '1933when',\n",
       " '1934',\n",
       " '19341938',\n",
       " '19341959br',\n",
       " '19345',\n",
       " '1934br',\n",
       " '1934s',\n",
       " '1935',\n",
       " '1935s',\n",
       " '1935very',\n",
       " '1936',\n",
       " '19361939',\n",
       " '19361945',\n",
       " '19361946',\n",
       " '19361959',\n",
       " '193637',\n",
       " '1936br',\n",
       " '1936in',\n",
       " '1936universal',\n",
       " '1937',\n",
       " '1937s',\n",
       " '1938',\n",
       " '1938s',\n",
       " '1939',\n",
       " '19391940',\n",
       " '193945',\n",
       " '1939and',\n",
       " '1939br',\n",
       " '1939i',\n",
       " '1939in',\n",
       " '1939robert',\n",
       " '1939s',\n",
       " '193os',\n",
       " '1940',\n",
       " '19402001',\n",
       " '194041',\n",
       " '1940br',\n",
       " '1940just',\n",
       " '1940s',\n",
       " '1940s1950sranging',\n",
       " '1940s60s',\n",
       " '1940sbr',\n",
       " '1940sbut',\n",
       " '1941',\n",
       " '19411945',\n",
       " '194145br',\n",
       " '1941br',\n",
       " '1941s',\n",
       " '1942',\n",
       " '1942br',\n",
       " '1942era',\n",
       " '1942s',\n",
       " '1942with',\n",
       " '1943',\n",
       " '19431945',\n",
       " '1943s',\n",
       " '1944',\n",
       " '194445',\n",
       " '1944early',\n",
       " '1944s',\n",
       " '1945',\n",
       " '19452000',\n",
       " '1945br',\n",
       " '1945film',\n",
       " '1946',\n",
       " '19461949',\n",
       " '194648',\n",
       " '194666',\n",
       " '1946br',\n",
       " '1946is',\n",
       " '1946s',\n",
       " '1947',\n",
       " '194748',\n",
       " '1947a',\n",
       " '1947br',\n",
       " ...]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_features = {value:pos for pos, value in enumerate(features_name)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "158461"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dictionary_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'a'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-106-58029b444133>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfirst_row_with_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-106-58029b444133>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfirst_row_with_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdictionary_features\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'review'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'a'"
     ]
    }
   ],
   "source": [
    "first_row_with_index = [dictionary_features[word] for word in df['review'][0].split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means counvectorizer strip some stopwords at least (above error).\n",
    "\n",
    "Easier way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Counter('I ama am am')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With counter you can create a term document frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Create the document-term matrix for the IMDb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In non-deep learning methods of NLP, we are often interested only in `which words` were used in a review, and `how often each word got used`. This is known as the `bag of words` approach, and it suggests a really simple way to store a document (in this case, a movie review). \n",
    "\n",
    "#### For each review we can keep track of which words were used and how often each word was used with a `vector` whose `length` is the number of tokens in the vocabulary, which we will call `n`. The `indexes` of this `vector` correspond to the `tokens` in the `IMDb vocabulary`, and the`values` of the vector are the number of times the corresponding tokens appeared in the review. For example the values stored at indexes 0, 1, 2, 3, 4 of the vector record the number of times the 5 tokens ['xxunk','xxpad','xxbos','xxeos','xxfld'] appeared in the review, respectively.\n",
    "\n",
    "#### Now, if our movie review database has `m` reviews, and each review is represented by a `vector` of length `n`, then vertically stacking the row vectors for all the reviews creates a matrix representation of the IMDb, which we call its `document-term matrix`. The `rows` correspond to `documents` (reviews), while the `columns` correspond to `terms` (or tokens in the vocabulary)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a term document matrix also with tf-idf from sklearn.\n",
    "\n",
    "You will get a matrix in which each row is a review.\n",
    "\n",
    "__Note:__\n",
    "\n",
    "In case you want to know the similarity between 2 documents you can apply the cosine similarity between the two vectors: <a href=\"https://stackoverflow.com/questions/11870210/tf-idf-simple-use-nltk-scikit-learn?rq=1\"> Article</a>. Cosine similarity = 1, when two vectors have same angle, instead angle 90 degress no relationship whatsover, and -1 is negative relationship."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(min_df=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save and Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "scipy.sparse.save_npz('term_doc_fre.npz', X_train_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tfidf = scipy.sparse.load_npz('term_doc_fre.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Sparse Matrix Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Even though we've reduced over 19,000 unique words in our corpus of reviews down to a vocabulary of 6,000 words, that's still a lot! But reviews are generally short, a few hundred words. So most tokens don't appear in a typical review.  That means that most of the entries in the document-term matrix will be zeros, and therefore ordinary matrix operations will waste a lot of compute resources multiplying and adding zeros. \n",
    "\n",
    "####  We want to maximize the use of space and time by storing and performing matrix operations on our document-term matrix as a **sparse matrix**. `scipy` provides tools for efficient sparse matrix representatin and operations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loosely speaking,  matrix with a high proportion of zeros is called `sparse` (the opposite of sparse is `dense`).  For sparse matrices, you can save a lot of memory by only storing the non-zero values.\n",
    "\n",
    "#### More specifically, a class of matrices is called **sparse** if the number of non-zero elements is proportional to the number of rows (or columns) instead of being proportional to the product rows x columns. An example is the class of diagonal matrices.\n",
    "\n",
    "\n",
    "<img src=\"images/sparse.png\" alt=\"floating point\" style=\"width: 30%\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing sparse matrix structure\n",
    "<img src=\"sparse-matrix-structure-visualization.png\" alt=\"floating point\" style=\"width: 90%\"/>\n",
    "ref. https://scipy-lectures.org/advanced/scipy_sparse/introduction.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sparse matrix storage formats\n",
    "\n",
    "<img src=\"summary_of_sparse_matrix_storage_schemes.png\" alt=\"floating point\" style=\"width: 90%\"/>\n",
    "ref. https://scipy-lectures.org/advanced/scipy_sparse/storage_schemes.html\n",
    "\n",
    "There are the most common sparse storage formats:\n",
    "- coordinate-wise (scipy calls COO)\n",
    "- compressed sparse row (CSR)\n",
    "- compressed sparse column (CSC)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition of the Compressed Sparse Row (CSR) format\n",
    "\n",
    "Let's start out with a presecription for the **CSR format** (ref. https://en.wikipedia.org/wiki/Sparse_matrix)\n",
    "\n",
    "Given a full matrix **`A`** that has **`m`** rows, **`n`** columns, and **`N`** nonzero values, the CSR (Compressed Sparse Row) representation uses three arrays as follows:\n",
    "\n",
    "1. **`Val[0:N]`** contains the **values** of the **`N` non-zero elements**.\n",
    "\n",
    "2. **`Col[0:N]`** contains the **column indices** of the **`N` non-zero elements**. \n",
    "    \n",
    "3. For each row **`i`** of **`A`**, **`RowPointer[i]`** contains the index in **Val** of the the first **nonzero value** in row **`i`**. If there are no nonzero values in the **ith** row, then **`RowPointer[i] = None`**. And, by convention, an extra value **`RowPointer[m] = N`** is tacked on at the end. \n",
    "\n",
    "Question: How many floats and ints does it take to store the matrix **`A`** in CSR format?\n",
    "\n",
    "Let's walk through [a few examples](http://www.mathcs.emory.edu/~cheung/Courses/561/Syllabus/3-C/sparse.html) at the Emory University website\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Store the document-term matrix in CSR format\n",
    "i.e. given the `TextList` object containing the list of reviews, return the three arrays (values, column_indices, row_pointer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scipy Implementation of sparse matrices\n",
    "\n",
    "From the [Scipy Sparse Matrix Documentation](https://docs.scipy.org/doc/scipy-0.18.1/reference/sparse.html)\n",
    "\n",
    "- To construct a matrix efficiently, use either dok_matrix or lil_matrix. The lil_matrix class supports basic slicing and fancy indexing with a similar syntax to NumPy arrays. As illustrated below, the COO format may also be used to efficiently construct matrices\n",
    "- To perform manipulations such as multiplication or inversion, first convert the matrix to either CSC or CSR format.\n",
    "- All conversions among the CSR, CSC, and COO formats are efficient, linear-time operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To really understand the CSR format, we need to be able know how to do two things:\n",
    "1. Translate a regular matrix A into CSR format\n",
    "2. Reconstruct a regular matrix from its CSR sparse representation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. What is a [Naive Bayes classifier](https://towardsdatascience.com/the-naive-bayes-classifier-e92ea9f47523)? \n",
    "\n",
    "\n",
    "also this article is pretty good: https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### The `bag of words model` considers a movie review as equivalent to a list of the counts of all the tokens that it contains. When you do this, you throw away the rich information that comes from the sequential arrangement of the tokens into sentences and paragraphs. \n",
    "\n",
    "#### Nevertheless, even if you are not allowed to read the review but are only given its representation as `token counts`, you can usually still get a pretty good sense of whether the review was good or bad. How do you do this?  By mentally gauging the overall `positive` or `negative` sentiment that the collection of words conveys, right?  \n",
    "\n",
    "#### The `Naive Bayes Classifier` is an algorithm that encodes this simple reasoning process mathematically. It is based on two important pieces of information that we can learn from the training set:\n",
    "* The `class priors`, i.e. the probabilities that a randomly chosen review will be `positive`, or `negative`\n",
    "* The `token likelihoods` i.e. how likely is it that a given token would appear in a `positive` or `negative` review \n",
    "\n",
    "#### It turns out that this is all the information we need to build a model capable of predicting fairly accurately how any given review will be classified, given its text! \n",
    "\n",
    "#### We shall unfold the complete explanation of the magic of the Naive Bayes Classifier in the next section. \n",
    "\n",
    "#### Meanwhile, In this section, we focus on how to compute the necessary information from the training data, specifically the `prior probabilities` for reviews of each class, and the `class occurrence counts` and `class likelihood ratios` for each `token` in the `vocabulary`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8A. Class priors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the training data we can determine the `class priors` $p$ and $q$, which are the overall probabilities that a randomly chosen review is in the `positive`, or `negative` class, resepectively. \n",
    "\n",
    "#### $p=\\frac{N^{+}}{N}$ \n",
    "#### and\n",
    "#### $q=\\frac{N^{-}}{N}$ \n",
    "\n",
    "#### Here $N^{+}$ and $N^{-}$ are the numbers of `positive` and `negative` reviews, and $N$ is the total number of reviews in the training set, so that \n",
    "\n",
    "#### $N = N^{+} + N^{-}$, \n",
    "\n",
    "#### and \n",
    "\n",
    "#### $q = 1-p$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8B. Class `occurrence counts`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let $C^{+}_{t}$ and $C^{-}_{t}$ be the `occurrence counts` of token $t$ in `positive` and `negative` reviews, respectively, and $N^{+}$ and $N^{-}$ be the total numbers of`positive` and `negative` reviews in the data set, respectively. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8C. Class likelihood ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then, given the knowledge that a review is classified as `positive`, the `conditional likelihood` that a token $t$ will appear in the review is\n",
    "### $ L(t|+) = \\frac{C^{+}_{t}}{N^+}$, \n",
    "#### and simlarly, the `conditional likelihood` of a token appearing in a `negative` review is \n",
    "### $ L(t|-) = \\frac{C^{-}_{t}}{N^-}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8D. The `log-count ratio`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the class likelihood ratios, we can define a **log-count ratio** $R_{t}$ for each token $t$ as\n",
    "### $ R_{t} = \\text{log} \\frac{L(t|+)}  {L(t|-)}$\n",
    "#### The `log-count ratio` ranks tokens by their relative affinities for positive and negative reviews\n",
    "#### We observe that\n",
    "* $R_{t} \\gt 0$ means `positive` reviews are more likely to contain this token \n",
    "* $R_{t} \\lt 0$ means `negative` reviews are more likely to contain this token \n",
    "* $R_{t} = 0$ indicates the token $t$ has equal likelihood to appear in  `positive` and `negative` reviews\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Building a Naive Bayes Classifier for IMDb movie reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the `occurrence count` arrays, we can compute the `class likelihoods` and `log-count ratios` of all the tokens in the vocabulary. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9A. Compute the `class likelihoods`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### We compute slightly modified `conditional likelihoods`, by adding 1 to the numerator and denominator to insure numerically stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1 = (C1+1) / ((y.items==positive).sum() + 1)\n",
    "L0 = (C0+1) / ((y.items==negative).sum() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9D.1 What is Bayes Theorem, and what does it have to say about IMDb movie reviews?\n",
    "\n",
    "Consider two events, $A$ and $B$  \n",
    "Then the probability of $A$ and $B$ occurring together can be written in two ways:\n",
    "$p(A,B) = p(A|B)\\cdot p(B)$\n",
    "$p(A,B) = p(B|A)\\cdot p(A)$\n",
    "\n",
    "where $p(A|B)$ and $p(B|A)$ are conditional probabilities:\n",
    "$p(A|B)$ is the probability of $A$ occurring given that $B$ has occurred,\n",
    "$p(A)$ is the probability that $A$ occurs,\n",
    "$p(B)$ is the probabilityt that $B$ occurs\n",
    "\n",
    "\n",
    "$\\textbf{Bayes Theorem}$ is just the statement that the right hand sides of the above two equations are equal:\n",
    "\n",
    "$p(A|B) \\cdot p(B) = p(B|A) \\cdot p(A)$\n",
    "\n",
    "Applying $\\textbf{Bayes Theorem}$ to our IMDb movie review problem:\n",
    "\n",
    "We identify $A$ and $B$ as <br> \n",
    "$A \\equiv \\text{class}$, i.e. positive or negative, and <br>\n",
    "$B \\equiv \\text{tokens}$, i.e. the \"bag\" of tokens used in the review\n",
    "\n",
    "Then $\\textbf{Bayes Theorem}$ says\n",
    "\n",
    "$p(\\text{class}|\\text{tokens})\\cdot p(\\text{tokens}) = p(\\text{tokens}|\\text{class}) \\cdot p(\\text{class})$\n",
    "\n",
    "so that <br>\n",
    "$p(\\text{class}|\\text{tokens}) = p(\\text{tokens}|\\text{class})\\cdot \\frac{p(\\text{class})}{p(\\text{tokens})}$\n",
    "\n",
    "Since $p(\\text{tokens})$ is a constant, we have the proportionality \n",
    "\n",
    "$p(\\text{class}|\\text{tokens}) \\propto p(\\text{tokens}|\\text{class})\\cdot p(\\text{class})$\n",
    "\n",
    "The left hand side of the above expression is called the $\\textbf{posterior class probability}$, the probability that the review is positive (or negative), given the tokens it contains. This is exactly what we want to predict!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9D.2 The Naive Bayes Classifier\n",
    "\n",
    "#### Given the list of tokens in a review, we seek to predict whether the review is rated as `positive` or `negative` \n",
    "\n",
    "#### We can make the prediction if we know the `posterior class probabilities`.\n",
    "\n",
    "#### $p(\\text{class}|\\text{tokens})$,\n",
    "#### where $\\text{class}$ is either `positive` or `negative`, and $\\text{tokens}$ is the list of tokens that appear in the review.\n",
    "#### [Bayes' Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) tells us that the posterior probabilities, the likelihoods and the priors are related this way:\n",
    "\n",
    "#### $p(\\text{class}|\\text{tokens}) \\propto p(\\text{tokens}|\\text{class})\\cdot p(\\text{class})$\n",
    "\n",
    "#### Now the tokens are not independent of one another.  For example, 'go' often appears with 'to', so if 'go' appears in a review it is more likely that the review also contains 'to'. Nevertheless, assuming the tokens are independent allows us to simplify things, so we recklessly do it, hoping it's not too wrong!\n",
    "#### $p(\\text{tokens}|\\text{class}) = \\prod_{i=1}^{n} p(t_{i}|\\text{class})$\n",
    "\n",
    "#### where $t_{i}$ is the $i\\text{th}$ token in the vocabulary and $n$ is the number of tokens in the vocabulary. \n",
    "\n",
    "#### So Bayes' theorem is\n",
    "\n",
    "#### $p(\\text{class}|\\text{tokens}) \\propto p(\\text{class}) \\prod_{i=1}^{n} p(t_{i}|\\text{class}) $\n",
    "\n",
    "#### Taking the ratio of the $\\textbf{posterior class probabilities}$ for the `positive` and `negative` classes, we have\n",
    "\n",
    "#### $\\frac{p(+|\\text{tokens})}{p( - |\\text{tokens})} =  \\frac{p(+)}{p( - )}  \\cdot  \\prod_{i=1}^{n} \\frac {p(t_{i}|+)}  {p(t_{i}| - )} = \\frac{p}{q}  \\cdot  \\prod_{i=1}^{n} \\frac {L(t_{i}|+)}  {L(t_{i}| - )}$\n",
    "#### since likelihoods are proportional to probabilities.\n",
    "#### Taking the log of both sides converts this to a `linear` problem:\n",
    "#### $\\text{log} \\frac{p(+|\\text{tokens})}{p( - |\\text{tokens})} = \\text{log}\\frac{p}{q} + \\sum_{i=1}^{n} \\text{log} \\frac {L(t_{i}|+)}  {L(t_{i}| - )} = b + \\sum_{i=1}^{n}  R_{t_{i}}$\n",
    "\n",
    "#### The first term on the right-hand side is the `bias`, and the second term is the dot product of the *binarized* embedding vector and the log-count ratios\n",
    "\n",
    "#### If the left-hand side is greater than or equal to zero, we predict the review is `positive`, else we predict the review is `negative`. \n",
    "\n",
    "####  We can re-write the last equation in matrix form to generate a $m \\times 1$ boolean column vector $\\textbf{preds}$ of review predictions:\n",
    "\n",
    "#### $\\textbf{preds} = \\textbf{W} \\cdot \\textbf{R} + \\textbf{b}$\n",
    "#### where \n",
    "\n",
    "* $\\textbf{preds} \\equiv \\text{log} \\frac{p(+|\\text{tokens})}{p( - |\\text{tokens})}$\n",
    "* $\\textbf{W}$ is the $m\\times n$ `binarized document-term matrix`, whose rows are the binarized embedding vectors for the movie reviews\n",
    "* $\\textbf{R}$ is the $n\\times 1$ vector of `log-count ratios`  for the tokens, and \n",
    "* $\\textbf{b}$ is a $n\\times 1$ vector whose entries are the bias $b$\n",
    "\n",
    "\n",
    "#### The Naive Bayes model consists of the log-counts vector $\\textbf{R}$ and the bias $\\textbf{b}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9E. Implement our Naive Bayes Movie Review classifier\n",
    "#### and use it to predict labels for the training and validation sets of the IMDb_sample data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_tfidf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-ccafaeb49ae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGaussianNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_tfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtodense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_train_tfidf' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "classifier = GaussianNB()\n",
    "classifier.fit(X_train_tfidf.todense(), y_train)\n",
    "y_pred = classifier.predict(X_train_tfidf.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40000,), (40000, 43611), 40000)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape, X_train_tfidf.shape, len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[19425,   541],\n",
       "       [ 4044, 15990]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.885375"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_pred == y_train).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "88% percent accuracy, not so bad! Implement also the binarized Naive Bayes a few sections below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9F. Summary: A recipe for the Naive Bayes  Classifier\n",
    "#### Here is a summary of our procedure for predicting labels with the Naive Bayes Classifier, starting with the training set `x` and the training labels `y`\n",
    "\n",
    "\n",
    "#### 1. Compute the token count vectors\n",
    "> C0 = np.squeeze(np.asarray(x[y.items==negative].sum(0))) <br> \n",
    "> C1 = np.squeeze(np.asarray(x[y.items==positive].sum(0))) <br> \n",
    "\n",
    "#### 2. Compute the token class likelihood vectors\n",
    "> L0 = (C0+1) / ((y.items==negative).sum() + 1) <br> \n",
    "> L1 = (C1+1) / ((y.items==positive).sum() + 1) <br> \n",
    "\n",
    "#### 3. Compute the log-count ratios vector\n",
    "> R = np.log(L1/L0)\n",
    "\n",
    "#### 4. Compute the bias term\n",
    "> b = np.log((y.items==positive).mean() / (y.items==negative).mean())\n",
    "\n",
    "#### 5. The Naive Bayes model consists of the log-counts vector $\\textbf{R}$ and the bias $\\textbf{b}$\n",
    "#### 6. Predict the movie review labels from a linear transformation of the log-count ratios vector:\n",
    "> preds = (W @ R + b) > 0, <br> \n",
    "> where the weights matrix W = valid_doc_term.sign() is the binarized `valid_doc_term matrix` whose rows are the binarized embedding vectors for the movie reviews for which you want to predict ratings.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Binarized Naive Bayes\n",
    "\n",
    "_Instead of having the count for each of the features (tf-idf) you binarize the term to 1 if is present or 0 if is not present. If \"cat\" was present 3 times in `doc_1` then you would have `cat = 3`, whereas with the binarized version you have `cat=1`, that's the idea behind this model._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.791575\n"
     ]
    }
   ],
   "source": [
    "classifier = GaussianNB()\n",
    "\n",
    "x = X_train_tfidf.sign().todense()\n",
    "y = y_train\n",
    "\n",
    "classifier.fit(x, y)\n",
    "preds = classifier.predict(x)\n",
    "accuracy_value_bin = (preds == y).mean()\n",
    "print(f'Accuracy {accuracy_value_bin}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Poor performance compared to a tf-idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Working with the full IMDb data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our approach working on a smaller sample of the data, we can try using it on the full dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10A. Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/data_clas.pkl'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/data_lm.pkl'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/finetuned.pth'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/finetuned_enc.pth'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/imdb.vocab'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/ld.pkl'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/ll_clas.pkl'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/ll_lm.pkl'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/models'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/pretrained'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/README'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/test'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/tmp_clas'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/tmp_lm'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/train'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/unsup'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/vocab_lm.pkl')]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.IMDB)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/train/labeledBow.feat'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/train/neg'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/train/pos'),\n",
       " WindowsPath('C:/Users/cross-entropy/.fastai/data/imdb/train/unsupBow.feat')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(path/'train').ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reviews_full.pickle', 'wb') as handle:\n",
    "    pickle.dump(reviews_full, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In the future, we'll just be able to load our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_doc_term = scipy.sparse.load_npz(\"train_doc_term.npz\")\n",
    "valid_doc_term = scipy.sparse.load_npz(\"valid_doc_term.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('reviews_full.pickle', 'rb') as handle:\n",
    "    pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. The Logistic Regression classifier with the full IMBb data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### With the `sci-kit learn` library, we can fit logistic a regression model where the features are the unigrams. Here $C$ is a regularization parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Using the full `document-term matrix`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy : 0.8769\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegression(C=0.1, dual=False, solver='liblinear')\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "preds = model.predict(X_train_tfidf)\n",
    "train_accuracy = (preds == y_train).mean()\n",
    "print(f'Train accuracy : {train_accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<40000x43611 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 5330339 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tfidf.sign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 0.8624\n"
     ]
    }
   ],
   "source": [
    "preds_test = model.predict(X_test_tfidf)\n",
    "test_accuracy = (preds_test == y_test).mean()\n",
    "print(f'Test accuracy : {test_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Ok, the result is not so bad. We have a 0.88 on train set and 0.87 on test set. It is 0.01 worst than __naive bayes__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. `Trigram` representation of the `IMDb_sample`: preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our next model is a version of logistic regression with Naive Bayes features extended to include bigrams and trigrams as well as unigrams, described [here](https://www.aclweb.org/anthology/P12-2018). For every document we compute binarized features as described above, but this time we use bigrams and trigrams too. Each feature is a log-count ratio. A logistic regression model is then trained to predict sentiment. Because of the much larger number of features, we will return to the smaller `IMDb_sample` data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are `ngrams`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### An `n-gram` is a contiguous sequence of n items (where the items can be characters, syllables, or words).  A `1-gram` is a `unigram`, a `2-gram` is a `bigram`, and a `3-gram` is a `trigram`.\n",
    "\n",
    "#### Here, we are referring to sequences of words. So examples of bigrams include \"the dog\", \"said that\", and \"can't you\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14A. How to know what keywords appear the most in the new matrix?\n",
    "\n",
    "Is not always easy to understand what the `CountVectorize` is, therefore can be useful to check out what the number of the matrix are. Let's see an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This is the second second document.',\n",
    "    'And the third one.',\n",
    "    'Is this the first document?',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1,2), min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0],\n",
       "        [0, 0, 1, 0, 0, 1, 1, 0, 0, 2, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0],\n",
       "        [0, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_bigram = bigram_vectorizer.fit_transform(corpus).todense(); x_train_bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(bigram_vectorizer.vocabulary_.get('this'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'this': 18,\n",
       " 'is': 5,\n",
       " 'the': 12,\n",
       " 'first': 3,\n",
       " 'document': 2,\n",
       " 'this is': 19,\n",
       " 'is the': 6,\n",
       " 'the first': 13,\n",
       " 'first document': 4,\n",
       " 'second': 9,\n",
       " 'the second': 14,\n",
       " 'second second': 11,\n",
       " 'second document': 10,\n",
       " 'and': 0,\n",
       " 'third': 16,\n",
       " 'one': 8,\n",
       " 'and the': 1,\n",
       " 'the third': 15,\n",
       " 'third one': 17,\n",
       " 'is this': 7,\n",
       " 'this the': 20}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All the vocabulary and combinations of bigrams\n",
    "bigram_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 3, 2, 2, 3, 2, 1, 1, 2, 1, 1, 4, 2, 1, 1, 1, 1, 3, 2, 1]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sum the appearences together\n",
    "x_train_bigram.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and',\n",
       " 'and the',\n",
       " 'document',\n",
       " 'first',\n",
       " 'first document',\n",
       " 'is',\n",
       " 'is the',\n",
       " 'is this',\n",
       " 'one',\n",
       " 'second',\n",
       " 'second document',\n",
       " 'second second',\n",
       " 'the',\n",
       " 'the first',\n",
       " 'the second',\n",
       " 'the third',\n",
       " 'third',\n",
       " 'third one',\n",
       " 'this',\n",
       " 'this is',\n",
       " 'this the']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort in order the words and after create the appereances vocabulary\n",
    "word_list_sorted = sorted(bigram_vectorizer.vocabulary_); word_list_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_list_sorted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 3, 2, 2, 3, 2, 1, 1, 2, 1, 1, 4, 2, 1, 1, 1, 1, 3, 2, 1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bg_train_count = x_train_bigram.sum(axis=0); x_bg_train_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.matrix"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_bg_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to reduce to a singular row\n",
    "x_bg_train_count = np.ravel(x_bg_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x_bg_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_bg_train_count[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = [{word:count} for word, count in zip(word_list_sorted, x_bg_train_count)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'and': 1},\n",
       " {'and the': 1},\n",
       " {'document': 3},\n",
       " {'first': 2},\n",
       " {'first document': 2},\n",
       " {'is': 3},\n",
       " {'is the': 2},\n",
       " {'is this': 1},\n",
       " {'one': 1},\n",
       " {'second': 2},\n",
       " {'second document': 1},\n",
       " {'second second': 1},\n",
       " {'the': 4},\n",
       " {'the first': 2},\n",
       " {'the second': 1},\n",
       " {'the third': 1},\n",
       " {'third': 1},\n",
       " {'third one': 1},\n",
       " {'this': 3},\n",
       " {'this is': 2},\n",
       " {'this the': 1}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14.B Naive Bayes with Bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1,2), max_features=80000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_bigram = bigram_vectorizer.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_bayes = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naive_bayes.fit(X_bigram.todense(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = naive_bayes.predict(X_bigram.todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92465"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_train == preds).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems crazy high, but should be correct!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14E. Save the `ngram` data so we won't have to spend the time to generate it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "scipy.sparse.save_npz(\"X_train_bigram_matrix.npz\", X_bigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('itongram.pickle', 'wb') as handle:\n",
    "    pickle.dump(itongram, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('ngramtoi.pickle', 'wb') as handle:\n",
    "    pickle.dump(ngramtoi, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### 14F. Load the `ngram` data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "train_ngram_doc_matrix = scipy.sparse.load_npz(\"train_ngram_matrix.npz\")\n",
    "valid_ngram_doc_matrix = scipy.sparse.load_npz(\"valid_ngram_matrix.npz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "with open('itongram.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)\n",
    "    \n",
    "with open('ngramtoi.pickle', 'rb') as handle:\n",
    "    b = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. A Naive Bayes IMDb classifier using Trigrams instead of Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try the exact same thing but with a trigram (computationally expensive), and see what is the output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Baselines and Bigrams: Simple, Good Sentiment and Topic Classification. Sida Wang and Christopher D. Manning [pdf](https://www.aclweb.org/anthology/P12-2018)\n",
    "* [The Naive Bayes Classifier](https://towardsdatascience.com/the-naive-bayes-classifier-e92ea9f47523). Joseph Catanzarite, in Towards Data Science"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
